vocabulary size: 120
n tunes: 3411
n train tunes: 3251.0
n validation tunes: 160.0
min, max length 2 1960
Building the model
  number of parameters: 5573816
  layer output shapes:               #params:   output shape:
    InputLayer                       0          (32, None)
    EmbeddingLayer                   14400      (32, None, 120)
    InputLayer                       0          (32, None)
    LSTMLayer                        1297408    (32, None, 512)
    DropoutLayer                     0          (32, None, 512)
    LSTMLayer                        2100224    (32, None, 512)
    DropoutLayer                     0          (32, None, 512)
    LSTMLayer                        2100224    (32, None, 512)
    DropoutLayer                     0          (32, None, 512)
    ReshapeLayer                     0          (None, 512)
    DenseLayer                       61560      (None, 120)
Train model
1/203 (epoch 0.010) train_loss=232.73118591 time/batch=1.22s
2/203 (epoch 0.020) train_loss=290.65158081 time/batch=2.01s
3/203 (epoch 0.030) train_loss=271.55688477 time/batch=1.47s
4/203 (epoch 0.039) train_loss=856.73638916 time/batch=4.73s
5/203 (epoch 0.049) train_loss=584.08691406 time/batch=3.38s
6/203 (epoch 0.059) train_loss=402.79794312 time/batch=2.67s
7/203 (epoch 0.069) train_loss=657.60028076 time/batch=4.78s
8/203 (epoch 0.079) train_loss=173.76144409 time/batch=1.51s
9/203 (epoch 0.089) train_loss=389.59310913 time/batch=6.74s
10/203 (epoch 0.098) train_loss=845.00457764 time/batch=16.49s
11/203 (epoch 0.108) train_loss=108.82350159 time/batch=1.94s
12/203 (epoch 0.118) train_loss=62.45563126 time/batch=1.14s
13/203 (epoch 0.128) train_loss=1269.76013184 time/batch=103.34s
14/203 (epoch 0.138) train_loss=319.20629883 time/batch=5.16s
15/203 (epoch 0.148) train_loss=352.39840698 time/batch=6.14s
16/203 (epoch 0.157) train_loss=598.01721191 time/batch=8.98s
17/203 (epoch 0.167) train_loss=353.24618530 time/batch=2.61s
18/203 (epoch 0.177) train_loss=482.77328491 time/batch=3.57s
19/203 (epoch 0.187) train_loss=542.24835205 time/batch=5.61s
20/203 (epoch 0.197) train_loss=489.47756958 time/batch=3.79s
21/203 (epoch 0.207) train_loss=332.33868408 time/batch=2.54s
22/203 (epoch 0.217) train_loss=177.92918396 time/batch=2.12s
23/203 (epoch 0.226) train_loss=256.75192261 time/batch=2.02s
24/203 (epoch 0.236) train_loss=263.37741089 time/batch=2.06s
25/203 (epoch 0.246) train_loss=199.18583679 time/batch=1.61s
26/203 (epoch 0.256) train_loss=706.16894531 time/batch=5.24s
27/203 (epoch 0.266) train_loss=288.15695190 time/batch=2.19s
28/203 (epoch 0.276) train_loss=310.68972778 time/batch=2.37s
29/203 (epoch 0.285) train_loss=51.57794571 time/batch=0.55s
30/203 (epoch 0.295) train_loss=302.40036011 time/batch=2.32s
31/203 (epoch 0.305) train_loss=322.60977173 time/batch=2.51s
32/203 (epoch 0.315) train_loss=594.02478027 time/batch=4.24s
33/203 (epoch 0.325) train_loss=376.45559692 time/batch=2.91s
34/203 (epoch 0.335) train_loss=379.20379639 time/batch=2.93s
35/203 (epoch 0.345) train_loss=189.54522705 time/batch=1.61s
36/203 (epoch 0.354) train_loss=273.02618408 time/batch=2.16s
37/203 (epoch 0.364) train_loss=558.95361328 time/batch=4.23s
38/203 (epoch 0.374) train_loss=176.78475952 time/batch=1.42s
39/203 (epoch 0.384) train_loss=114.32135010 time/batch=1.03s
40/203 (epoch 0.394) train_loss=495.97921753 time/batch=3.72s
41/203 (epoch 0.404) train_loss=524.63757324 time/batch=4.00s
42/203 (epoch 0.413) train_loss=512.12103271 time/batch=3.90s
43/203 (epoch 0.423) train_loss=215.19050598 time/batch=1.76s
44/203 (epoch 0.433) train_loss=274.34008789 time/batch=2.28s
45/203 (epoch 0.443) train_loss=175.87445068 time/batch=1.50s
46/203 (epoch 0.453) train_loss=535.51208496 time/batch=4.05s
47/203 (epoch 0.463) train_loss=639.89520264 time/batch=4.93s
48/203 (epoch 0.472) train_loss=453.74533081 time/batch=3.39s
49/203 (epoch 0.482) train_loss=441.37716675 time/batch=3.42s
50/203 (epoch 0.492) train_loss=269.86425781 time/batch=2.24s
51/203 (epoch 0.502) train_loss=412.50485229 time/batch=3.19s
52/203 (epoch 0.512) train_loss=232.45933533 time/batch=1.87s
53/203 (epoch 0.522) train_loss=339.64266968 time/batch=2.66s
54/203 (epoch 0.532) train_loss=275.05416870 time/batch=2.22s
55/203 (epoch 0.541) train_loss=366.37051392 time/batch=2.94s
56/203 (epoch 0.551) train_loss=458.98568726 time/batch=3.68s
57/203 (epoch 0.561) train_loss=421.93450928 time/batch=3.27s
58/203 (epoch 0.571) train_loss=68.27849579 time/batch=0.71s
59/203 (epoch 0.581) train_loss=458.30828857 time/batch=3.45s
60/203 (epoch 0.591) train_loss=485.03262329 time/batch=3.80s
61/203 (epoch 0.600) train_loss=67.67758942 time/batch=0.88s
62/203 (epoch 0.610) train_loss=271.30749512 time/batch=2.21s
63/203 (epoch 0.620) train_loss=240.54818726 time/batch=1.93s
64/203 (epoch 0.630) train_loss=138.98681641 time/batch=12.26s
65/203 (epoch 0.640) train_loss=168.67182922 time/batch=1.54s
66/203 (epoch 0.650) train_loss=359.60009766 time/batch=2.77s
67/203 (epoch 0.659) train_loss=186.08195496 time/batch=2.12s
68/203 (epoch 0.669) train_loss=235.41288757 time/batch=1.96s
69/203 (epoch 0.679) train_loss=193.34536743 time/batch=1.68s
70/203 (epoch 0.689) train_loss=198.73318481 time/batch=1.74s
71/203 (epoch 0.699) train_loss=422.07217407 time/batch=3.34s
72/203 (epoch 0.709) train_loss=171.22781372 time/batch=2.14s
73/203 (epoch 0.719) train_loss=387.49353027 time/batch=3.07s
74/203 (epoch 0.728) train_loss=508.43164062 time/batch=4.12s
75/203 (epoch 0.738) train_loss=232.97901917 time/batch=2.00s
76/203 (epoch 0.748) train_loss=342.16946411 time/batch=2.71s
77/203 (epoch 0.758) train_loss=202.35339355 time/batch=1.71s
78/203 (epoch 0.768) train_loss=360.03656006 time/batch=2.96s
79/203 (epoch 0.778) train_loss=306.08203125 time/batch=2.47s
80/203 (epoch 0.787) train_loss=299.48144531 time/batch=2.45s
81/203 (epoch 0.797) train_loss=332.26644897 time/batch=2.77s
82/203 (epoch 0.807) train_loss=312.04364014 time/batch=2.52s
83/203 (epoch 0.817) train_loss=383.58685303 time/batch=3.08s
84/203 (epoch 0.827) train_loss=200.62397766 time/batch=1.83s
85/203 (epoch 0.837) train_loss=333.23727417 time/batch=2.78s
86/203 (epoch 0.847) train_loss=441.06564331 time/batch=3.76s
87/203 (epoch 0.856) train_loss=216.24082947 time/batch=1.96s
88/203 (epoch 0.866) train_loss=242.65509033 time/batch=4.24s
89/203 (epoch 0.876) train_loss=173.91122437 time/batch=3.70s
90/203 (epoch 0.886) train_loss=330.08117676 time/batch=32.20s
91/203 (epoch 0.896) train_loss=194.58480835 time/batch=4.40s
92/203 (epoch 0.906) train_loss=302.72134399 time/batch=6.26s
93/203 (epoch 0.915) train_loss=367.41000366 time/batch=7.40s
94/203 (epoch 0.925) train_loss=225.29568481 time/batch=4.67s
95/203 (epoch 0.935) train_loss=240.08709717 time/batch=4.92s
96/203 (epoch 0.945) train_loss=262.48553467 time/batch=5.50s
97/203 (epoch 0.955) train_loss=287.17984009 time/batch=5.61s
98/203 (epoch 0.965) train_loss=281.93481445 time/batch=5.60s
99/203 (epoch 0.974) train_loss=396.93569946 time/batch=7.65s
100/203 (epoch 0.984) train_loss=384.75555420 time/batch=7.45s
101/203 (epoch 0.994) train_loss=334.44503784 time/batch=6.61s
  saved to metadata/configSample--20250107-204945.pkl
102/203 (epoch 1.004) train_loss=758.91937256 time/batch=15.17s
103/203 (epoch 1.014) train_loss=664.70629883 time/batch=12.55s
104/203 (epoch 1.024) train_loss=176.15429688 time/batch=3.61s
105/203 (epoch 1.034) train_loss=609.61761475 time/batch=11.70s
106/203 (epoch 1.043) train_loss=393.40615845 time/batch=7.54s
107/203 (epoch 1.053) train_loss=334.29519653 time/batch=6.63s
108/203 (epoch 1.063) train_loss=285.15316772 time/batch=5.32s
109/203 (epoch 1.073) train_loss=509.47021484 time/batch=9.35s
110/203 (epoch 1.083) train_loss=143.64201355 time/batch=14.92s
111/203 (epoch 1.093) train_loss=533.04333496 time/batch=4.19s
112/203 (epoch 1.102) train_loss=1203.50585938 time/batch=43.34s
113/203 (epoch 1.112) train_loss=521.32556152 time/batch=11.30s
114/203 (epoch 1.122) train_loss=157.42852783 time/batch=1.41s
115/203 (epoch 1.132) train_loss=380.75546265 time/batch=3.09s
116/203 (epoch 1.142) train_loss=498.72250366 time/batch=4.01s
117/203 (epoch 1.152) train_loss=72.09630585 time/batch=0.80s
118/203 (epoch 1.161) train_loss=631.18908691 time/batch=5.29s
119/203 (epoch 1.171) train_loss=46.23554611 time/batch=0.60s
120/203 (epoch 1.181) train_loss=55.68654251 time/batch=0.68s
121/203 (epoch 1.191) train_loss=177.78918457 time/batch=1.67s
122/203 (epoch 1.201) train_loss=117.37651825 time/batch=1.20s
123/203 (epoch 1.211) train_loss=443.60336304 time/batch=3.53s
124/203 (epoch 1.221) train_loss=443.21963501 time/batch=3.48s
125/203 (epoch 1.230) train_loss=276.70950317 time/batch=2.31s
126/203 (epoch 1.240) train_loss=395.08596802 time/batch=3.17s
127/203 (epoch 1.250) train_loss=229.37597656 time/batch=2.00s
128/203 (epoch 1.260) train_loss=459.80096436 time/batch=3.67s
129/203 (epoch 1.270) train_loss=30.87223244 time/batch=0.47s
130/203 (epoch 1.280) train_loss=284.83258057 time/batch=2.45s
131/203 (epoch 1.289) train_loss=563.09454346 time/batch=4.59s
132/203 (epoch 1.299) train_loss=237.19970703 time/batch=2.04s
133/203 (epoch 1.309) train_loss=471.21975708 time/batch=3.78s
134/203 (epoch 1.319) train_loss=353.93569946 time/batch=2.93s
135/203 (epoch 1.329) train_loss=523.22772217 time/batch=4.13s
136/203 (epoch 1.339) train_loss=542.53088379 time/batch=4.27s
137/203 (epoch 1.349) train_loss=398.49597168 time/batch=3.35s
138/203 (epoch 1.358) train_loss=76.07750702 time/batch=1.05s
139/203 (epoch 1.368) train_loss=378.88903809 time/batch=3.07s
140/203 (epoch 1.378) train_loss=557.34704590 time/batch=4.40s
141/203 (epoch 1.388) train_loss=182.33520508 time/batch=1.65s
142/203 (epoch 1.398) train_loss=276.19213867 time/batch=2.36s
143/203 (epoch 1.408) train_loss=170.95982361 time/batch=1.60s
144/203 (epoch 1.417) train_loss=186.99066162 time/batch=1.71s
145/203 (epoch 1.427) train_loss=463.88864136 time/batch=3.58s
146/203 (epoch 1.437) train_loss=135.97746277 time/batch=1.29s
147/203 (epoch 1.447) train_loss=209.79426575 time/batch=1.83s
148/203 (epoch 1.457) train_loss=155.11938477 time/batch=1.43s
149/203 (epoch 1.467) train_loss=209.51730347 time/batch=1.89s
150/203 (epoch 1.476) train_loss=466.95111084 time/batch=3.79s
151/203 (epoch 1.486) train_loss=293.44824219 time/batch=2.44s
152/203 (epoch 1.496) train_loss=263.64312744 time/batch=2.36s
153/203 (epoch 1.506) train_loss=331.97760010 time/batch=2.82s
154/203 (epoch 1.516) train_loss=242.22427368 time/batch=2.14s
155/203 (epoch 1.526) train_loss=502.89367676 time/batch=4.41s
156/203 (epoch 1.536) train_loss=103.98468781 time/batch=1.07s
157/203 (epoch 1.545) train_loss=334.85864258 time/batch=31.74s
158/203 (epoch 1.555) train_loss=331.76617432 time/batch=6.48s
159/203 (epoch 1.565) train_loss=308.82449341 time/batch=6.19s
160/203 (epoch 1.575) train_loss=319.14971924 time/batch=6.27s
161/203 (epoch 1.585) train_loss=88.74481201 time/batch=2.84s
162/203 (epoch 1.595) train_loss=358.83151245 time/batch=7.05s
163/203 (epoch 1.604) train_loss=198.12844849 time/batch=5.05s
164/203 (epoch 1.614) train_loss=316.93316650 time/batch=6.37s
165/203 (epoch 1.624) train_loss=166.21345520 time/batch=5.05s
166/203 (epoch 1.634) train_loss=196.94934082 time/batch=4.16s
167/203 (epoch 1.644) train_loss=301.98254395 time/batch=6.09s
168/203 (epoch 1.654) train_loss=191.15228271 time/batch=4.02s
169/203 (epoch 1.663) train_loss=252.43180847 time/batch=5.23s
170/203 (epoch 1.673) train_loss=234.68095398 time/batch=4.97s
171/203 (epoch 1.683) train_loss=425.28353882 time/batch=7.98s
172/203 (epoch 1.693) train_loss=475.91082764 time/batch=10.25s
173/203 (epoch 1.703) train_loss=352.93847656 time/batch=7.06s
174/203 (epoch 1.713) train_loss=307.69601440 time/batch=6.01s
175/203 (epoch 1.723) train_loss=229.84553528 time/batch=4.63s
176/203 (epoch 1.732) train_loss=238.23307800 time/batch=5.10s
177/203 (epoch 1.742) train_loss=304.72817993 time/batch=5.97s
178/203 (epoch 1.752) train_loss=426.30038452 time/batch=7.99s
179/203 (epoch 1.762) train_loss=166.01586914 time/batch=3.61s
180/203 (epoch 1.772) train_loss=388.05007935 time/batch=7.71s
181/203 (epoch 1.782) train_loss=296.84460449 time/batch=6.03s
182/203 (epoch 1.791) train_loss=422.60525513 time/batch=8.24s
183/203 (epoch 1.801) train_loss=186.91621399 time/batch=4.21s
184/203 (epoch 1.811) train_loss=282.41140747 time/batch=6.20s
185/203 (epoch 1.821) train_loss=231.54177856 time/batch=5.12s
186/203 (epoch 1.831) train_loss=279.27966309 time/batch=5.63s
187/203 (epoch 1.841) train_loss=335.94573975 time/batch=6.78s
188/203 (epoch 1.851) train_loss=416.72811890 time/batch=7.84s
189/203 (epoch 1.860) train_loss=214.56790161 time/batch=4.40s
190/203 (epoch 1.870) train_loss=161.88314819 time/batch=4.30s
191/203 (epoch 1.880) train_loss=356.16909790 time/batch=7.58s
192/203 (epoch 1.890) train_loss=312.15097046 time/batch=6.22s
193/203 (epoch 1.900) train_loss=309.21273804 time/batch=6.40s
194/203 (epoch 1.910) train_loss=218.86267090 time/batch=4.66s
195/203 (epoch 1.919) train_loss=196.22344971 time/batch=4.34s
196/203 (epoch 1.929) train_loss=249.36712646 time/batch=5.20s
197/203 (epoch 1.939) train_loss=331.06237793 time/batch=6.67s
198/203 (epoch 1.949) train_loss=213.21212769 time/batch=4.52s
199/203 (epoch 1.959) train_loss=256.98437500 time/batch=5.33s
200/203 (epoch 1.969) train_loss=347.75585938 time/batch=6.72s
201/203 (epoch 1.978) train_loss=252.23541260 time/batch=5.22s
202/203 (epoch 1.988) train_loss=275.55838013 time/batch=6.67s
  saved to metadata/configSample--20250107-204945.pkl
